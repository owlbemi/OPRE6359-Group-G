---
title: 'OPRE6359 Group G Final Project'
subtitle: 'MIS6356.004'
author: "Jake Lee, Mishal Andoor, Vinisha Gunasaleen, Jayveenrhaj Gunasaleen, Nivas Annamareddy, Andrea Ontiveros"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
mainfont: "JetBrains Mono"
---
# Config Git
``git config --global user.name "Your Name"``
``git config --global user.email "your.email@example.com"``

# Required packages

```{r}
# run remotes::install_github("benyamindsmith/RKaggle")
library(RKaggle)
library(dplyr)
library(tidyr)
library(caret)
library(rpart)
library(rpart.plot)
```

# Importing Data

Reference: https://www.kaggle.com/datasets/sahilislam007/ai-impact-on-job-market-20242030

```{r}
raw_df <- get_dataset("sahilislam007/ai-impact-on-job-market-20242030")
```

# Basic Data Cleaning

```{r}
summary(raw_df)

# factorise certain columns

df <- raw_df

cols <- c("Job Status", "Required Education", "AI Impact Level", "Industry")
df[cols] <- lapply(raw_df[cols], as.factor)

# order the factors

df$`AI Impact Level` <- factor(
  df$`AI Impact Level`,
  levels = c("Low", "Moderate", "High"),
  ordered = TRUE
)

df$`Required Education` <- factor(
  df$`Required Education`,
  levels = c("High School", "Associate Degree", "Bachelor’s Degree", "Master’s Degree", "PhD"),
  ordered = TRUE
)

```


```{r}
head(df)

summary(df)
```

```{r}
names(df)
```

# Decision Tree
## Selecting Features

```{r}

# Keep only numeric predictors + target
df_numeric <- df[, c(
  "Job Status",
  "Median Salary (USD)",
  "Experience Required (Years)",
  "Job Openings (2024)",
  "Projected Openings (2030)",
  "Remote Work Ratio (%)",
  "Automation Risk (%)",
  "Gender Diversity (%)"
)]

# Check structure
str(df_numeric)

```
# Plotting and fitting Decision Tree

```{r}

# Split into training and test sets (2/3 training, 1/3 test)
set.seed(345)
train_index <- sample(1:nrow(df_numeric), nrow(df_numeric) * (2/3))
train_data <- df_numeric[train_index, ]
test_data  <- df_numeric[-train_index, ]

# Fit the decision tree
fit_numeric <- rpart(
  `Job Status` ~ ., 
  data = train_data,
  method = "class",
  control = rpart.control(minsplit = 200, cp = 0.001, maxdepth = 4),
  parms = list(split = "gini")
)

# Check the fit
print(fit_numeric)
```
## Accuracy 

```{r}

# Plot the tree
rpart.plot(fit_numeric, type = 1, extra = 2, cex = 0.6)

# Predict on test data and create confusion matrix
jobstatus.pred <- predict(fit_numeric, newdata = test_data, type = "class")
jobstatus.actual <- test_data$`Job Status`
confusion.matrix <- table(jobstatus.pred, jobstatus.actual)
confusion.matrix

# Accuracy on Training Data
jobstatus.pred <- predict(fit_numeric, train_data, type = "class")
jobstatus.actual <- train_data$`Job Status`
train_accuracy <- sum(jobstatus.pred == jobstatus.actual) / nrow(train_data)
print(paste("Training Accuracy:", round(train_accuracy, 3)))


# Accuracy on Testing Data
jobstatus.pred <- predict(fit_numeric, test_data, type = "class")
jobstatus.actual <- test_data$`Job Status`
test_accuracy <- sum(jobstatus.pred == jobstatus.actual) / nrow(test_data)
print(paste("Testing Accuracy:", round(test_accuracy, 3)))

```
# Logistic Regression
## Train / Test Data
  
```{r}
# set random seed for ML
set.seed(2021)

train_index <- sample(1:nrow(df), 0.6 * nrow(df))

train.df <- df[train_index, ]
test.df <- df[-train_index, ]

# Run Logreg

logit.reg <- glm(`Job Status` ~ `AI Impact Level` + `Median Salary (USD)` + `Required Education` + `Experience Required (Years)` + `Job Openings (2024)` + `Projected Openings (2030)` + `Remote Work Ratio (%)` + `Automation Risk (%)` + `Gender Diversity (%)`, 
                 data = train.df, 
                 family = "binomial")

summary(logit.reg)
```

## Prediction

```{r}
logitPredict <- predict(logit.reg, test.df, type = "response")

summary(logitPredict)

logitPredictClass <- ifelse(logitPredict > 0.5, 1, 0)
```

## Confusion Matrix

```{r}
actual <- test.df$`Job Status`
predict <- logitPredictClass
cm <- table(predict, actual)

cm
```

